{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Import Pipeline from transformers to perform various Tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "classifier=pipeline(\"sentiment-analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tasks=classifier([\n",
        "    \"I'm learning Hugging Face course nowadays and this course is very great and i've learned a lot from this course\",\n",
        "    \"But Balancing work and learning has been quite exhausting lately\"\n",
        "])\n",
        "print(tasks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Check labels and their Scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for task in tasks:\n",
        "  Labels=task[\"label\"]\n",
        "  Scores=task[\"score\"]\n",
        "  print(\"Labels: \",Labels, '---->', \"Score: \", Scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Initializing the Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **preprocessing with tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tasks=tokenizer([\n",
        "    \"I'm learning Hugging Face course nowadays and this course is very great and i've learned a lot from this course\",\n",
        "    \"But Balancing work and learning has been quite exhausting lately\"\n",
        "])\n",
        "print(tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(tasks[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for key in tasks:\n",
        "  print(\"Input ids\", tasks[\"input_ids\"])\n",
        "  print(\"keys\", key)\n",
        "  print(\"Token type ids\", tasks[\"token_type_ids\"])\n",
        "  print(\"Attention Mask\", tasks[\"attention_mask\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_text=[\n",
        "    \"I just completed my first NLP model thanks to this course, and it feels amazing!\",\n",
        "    \"This course is structured so well, it's the best learning experience I've had online.\",\n",
        "    \"The pace is too fast, and I feel completely lost after the first few lessons.\"\n",
        "]\n",
        "input=tokenizer(raw_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for key in input:\n",
        "  print(\"Input ids\", input[\"input_ids\"])\n",
        "  print(\"keys\", key)\n",
        "  print(\"Token type ids\", input[\"token_type_ids\"])\n",
        "  print(\"Attention Mask\", input[\"attention_mask\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Initializing the AutoModelForSequenceClassification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoModelForSequenceClassification\n",
        "model=AutoModel.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Model Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output=model(**input)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Let's Check the Hidden State**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output.last_hidden_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Loading Tokenizer and Model for Sequence Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "new_model=AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "print(new_tokenizer(raw_text))\n",
        "new_output=new_model(**input)\n",
        "print(new_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_output.logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yRnNZ9B2zRB"
      },
      "source": [
        "### **Pipeline Step#2: Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoConfig\n",
        "bert_config=AutoConfig.from_pretrained(\"bert-base-cased\")\n",
        "distill_bert_config=AutoConfig.from_pretrained(\"distilbert-base-cased\")\n",
        "print(\"Bert Config\")\n",
        "print(bert_config)\n",
        "print(\"====================================\")\n",
        "print(\"DistillBert Config\")\n",
        "print(distill_bert_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Build and Initiate Bert Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertModel\n",
        "# building the config\n",
        "bert_model_config=BertConfig()\n",
        "# initiate the model\n",
        "new_bert_model=BertModel(bert_model_config)\n",
        "output=new_bert_model(**input)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Bert Model Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert_model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_bert_model=BertModel.from_pretrained(\"bert-base-cased\", num_hidden_layers= 12, pad_token_id= -1)\n",
        "output=new_bert_model(**input)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert_model=AutoModel.from_pretrained(\"bert-base-cased\")\n",
        "new_output=bert_model(**input)\n",
        "print(new_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_output.last_hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert_model.save_pretrained(\"/content/bert_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6wwDwfUE9CC"
      },
      "source": [
        "### **Using a Transformer model for inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sequences = [\"Hello!\", \"Cool\", \"Nice!\"]\n",
        "\n",
        "encoded_sequences = [\n",
        "    [101, 7592, 999, 102],\n",
        "    [101, 4658, 1012, 102],\n",
        "    [101, 3835, 999, 102],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "new_input=torch.tensor(encoded_sequences)\n",
        "print(new_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model=AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "google_bert_output=model(new_input)\n",
        "print(google_bert_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "google_bert_output.logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "probabilities=torch.nn.functional.softmax(google_bert_output.logits, dim=-1)\n",
        "print(probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkm4zdXdcKrm"
      },
      "source": [
        "## **Tokenizers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxqIYZ6Adm5V"
      },
      "source": [
        "### **Word Tokenize**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "tokenize_text=\"I'm doing Datascience and ai projects, and now a days I'm doing NLP course\"\n",
        "text_tokenize=tokenize_text.split()\n",
        "print(text_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "output_text=tokenizer.tokenize(tokenize_text)\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmGKBdIffipn"
      },
      "source": [
        "### **Loading and saving**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert_base_model=AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
        "output=bert_base_model(**input)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "outputs = tokenizer(\"Using a Transformer network is simple\")\n",
        "for key,value in outputs.items():\n",
        "  print(\"key: \", key)\n",
        "  print(\"value: \", value, end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Save bert-base-cased Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert_base_model.save_pretrained(\"/content/bert_base_new_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNL3gOQIgSrx"
      },
      "source": [
        "### **Convert tokens to input ids**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids=tokenizer.convert_tokens_to_ids(output_text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decode_ids=tokenizer.decode([146, 112, 182, 1833, 7154, 22274, 1105, 170, 1182, 3203, 117, 1105, 1208, 170, 1552, 146, 112, 182, 1833, 21239, 2101, 1736])\n",
        "print(decode_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"IDs: \", ids)\n",
        "print(\"Input IDs: \", tokenizer(output_text)[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otkFUo1IiyXH"
      },
      "source": [
        "### **convert ids to tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.convert_ids_to_tokens([146, 112, 182, 1833, 7154, 22274, 1105, 170, 1182, 3203, 117, 1105, 1208, 170, 1552, 146, 112, 182, 1833, 21239, 2101, 1736])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Summing Up All Together**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "checkpoint=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model=AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "raw_inputs=[\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I have learned a lot from this course!\",\n",
        "]\n",
        "inputs=tokenizer.tokenize(raw_inputs)\n",
        "print(inputs)\n",
        "# convert tokens to ids\n",
        "ids = tokenizer.convert_tokens_to_ids(inputs)\n",
        "print(ids)\n",
        "input_ids=torch.tensor(ids)\n",
        "print(input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Performing Inference with istilbert-base-uncased-finetuned-sst-2-english Pre-trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I have learned a lot from this course!\",\n",
        "]\n",
        "\n",
        "# Proper tokenization with batching\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "output=model(**inputs)\n",
        "\n",
        "# Output logits\n",
        "print(output.logits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "padding_id=100\n",
        "batched_id=[[200,200,200],\n",
        "            [200,200,padding_id]]\n",
        "checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model=AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequence_1_ids=[[200,200,200]]\n",
        "sequence_2_ids=[[200,200,200],\n",
        "            [200,200,tokenizer.pad_token_id]]\n",
        "print(model(torch.tensor(sequence_1_ids)))\n",
        "print(model(torch.tensor(sequence_2_ids)))\n",
        "print(model(torch.tensor(batched_id)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_sequence=[\"I'm doing Huggig Face course\",\n",
        "               \"It's content is very good\"]\n",
        "input=tokenizer(raw_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Check Attention Mask and Input ids**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for key in input:\n",
        "  print(key)\n",
        "  print(input[key])\n",
        "  print(input['attention_mask'])\n",
        "  print(input['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.model_max_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Padding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_model_input=tokenizer(text_sequence, padding=\"max_length\")\n",
        "print(new_model_input)\n",
        "new_model_input_1=tokenizer(text_sequence, padding=\"longest\")\n",
        "print(new_model_input_1)\n",
        "new_model_input_2=tokenizer(text_sequence, padding=\"max_length\", max_length=8)\n",
        "print(new_model_input_2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
